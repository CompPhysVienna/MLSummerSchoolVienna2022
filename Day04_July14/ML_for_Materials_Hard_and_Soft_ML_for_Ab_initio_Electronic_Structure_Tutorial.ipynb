{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML for Materials Hard and Soft - ML for Ab-initio Electronic Structure Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halvarsu/MLSummerSchoolVienna2022/blob/add_problems/Day04_July14/ML_for_Materials_Hard_and_Soft_ML_for_Ab_initio_Electronic_Structure_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning for _Ab-initio_ Electronic Structure Tutorial\n",
        "\n",
        "This tutorial is for the [ML for Materials Hard and Soft](https://physik.univie.ac.at/events/events-detailansicht/news/machine-learning-for-materials-hard-and-soft-esi-dcafm-taco-vdsp-summer-school-2022/) summer school at the University of Vienna in July 2022.\n",
        "\n",
        "This version of the colab contains problem statements only, and won't run on its own. A version with solutions can be found [here](https://colab.research.google.com/github/CompPhysVienna/MLSummerSchoolVienna2022/blob/main/Day04_July14/ML_for_Materials_Hard_and_Soft_ML_for_Ab_initio_Electronic_Structure_Tutorial_(Solution).ipynb)."
      ],
      "metadata": {
        "id": "nfskFv0mWiMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Â© The Authors. Apache License 2.0.\n",
        "\n",
        "Halvard Sutterud (Imperial College London)\n",
        "\n",
        "David Pfau, James Spencer (DeepMind)"
      ],
      "metadata": {
        "id": "QD6hZzvnY4x4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outline\n",
        "\n",
        "In this tutorial, we hope to give you an introduction for how to implement a neural network wavefunction ansatz in JAX. We'll give a very short introduction to JAX, then walk you through how to create a simple neural network, evaluate the energy and gradient of the energy for simple systems, how to sample from the wavefunction squared, and finally how to train the neural network by variational Monte Carlo."
      ],
      "metadata": {
        "id": "T-tGEFr1iZSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "saruq3HNakAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main package we will be using is [JAX](https://jax.readthedocs.io/), a library for automatic differentiation on advanced hardware (like GPUs and TPUs) which is closely modeled on NumPy and SciPy. While JAX is ideal for working with this advanced hardware, nothing in this tutorial requires anything more than a standard CPU backend. JAX, NumPy and many other packages come installed by default in Colab. For this tutorial we'll use [Optax](https://optax.readthedocs.io/) so that we don't have to write optimizers like ADAM by hand. We also will be using [PySCF](https://pyscf.org/) to run reference calculations using more traditional electronic structure methods, and [PyBlock](https://pyblock.readthedocs.io/) to analyze the results.\n",
        "\n",
        "To run the code in these cells, just hit `shift`+`enter`. Then JAX should import without issue."
      ],
      "metadata": {
        "id": "zkroh2mSSLXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optax > /dev/null 2>&1  # optax is a library for optimization based on jax\n",
        "!pip install pyscf > /dev/null 2>&1  # pyscf is for calculating reference energies\n",
        "!pip install pyblock > /dev/null 2>&1  # pyblock is for error analysis\n",
        "\n",
        "import jax\n",
        "jax.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f4rplYqTiYzi",
        "outputId": "50950607-3e31-4a9f-88ee-c5ad3d5fe77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.3.14'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX Introduction"
      ],
      "metadata": {
        "id": "kd_8gEP3DV6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This whirlwhind tour through JAX will hopefully be enough to get started on the problems in the rest of this tutorial. The JAX documentation itself has many fantastic additional resources for learning about the ins and outs of the package:\n",
        "\n",
        "https://jax.readthedocs.io/en/latest/jax-101/index.html\n",
        "\n",
        "https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html\n",
        "\n",
        "https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html"
      ],
      "metadata": {
        "id": "dRLv8oCKC2mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most standard numerical operations in JAX can be found in the `jax.numpy` directory. This is a 1:1 implementation of the NumPy interface. In a few cases, there are extra arguments in the JAX implementation, but everything that exists in NumPy should also exist in `jax.numpy` (though there are a very small number of edge cases that should fail with a warning). There is also a `jax.scipy` directory which contains implementations of many special functions in SciPy, though it is not as complete as `jax.numpy`. There are also several special linear algebra operations not available in either which can be found in `jax.lax` , but we will not need those in this tutorial."
      ],
      "metadata": {
        "id": "0CGT_brfxUlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp  # the standard abbreviation, to avoid confusion with numpy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "uP1nrsH_DV6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Arrays"
      ],
      "metadata": {
        "id": "FDuLika-Kr1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arrays work basically the same..."
      ],
      "metadata": {
        "id": "pj19uq5yIGXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_np = np.array([1, 2, 3])\n",
        "x_jax = jnp.array([1, 2, 3])\n",
        "\n",
        "print(x_np)\n",
        "print(x_jax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjslCsEuIJzA",
        "outputId": "6cc8c630-ee0b-45ab-95b1-302979691174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3]\n",
            "[1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "...but have different types"
      ],
      "metadata": {
        "id": "kbX5nfxOITFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(x_np))\n",
        "print(type(x_jax))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBATBJCwIQkM",
        "outputId": "92f7f49d-f023-466f-d394-489e2057691d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'jaxlib.xla_extension.DeviceArray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "-QVDALKEKt0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions written in NumPy can often be replaced line-for-line with JAX"
      ],
      "metadata": {
        "id": "-EE86JWUIbKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f_np(x):\n",
        "  y = 2 * x\n",
        "  z = x + 1\n",
        "  return jnp.dot(np.log(y), z)\n",
        "\n",
        "def f_jax(x):\n",
        "  y = 2 * x\n",
        "  z = x + 1\n",
        "  return jnp.dot(jnp.log(y), z)"
      ],
      "metadata": {
        "id": "U-v1efRoIhTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f_np(x_np))\n",
        "print(f_jax(x_jax))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFAx52IMItJY",
        "outputId": "e48c144f-6131-4a5f-e212-73ba6ed573aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.712215\n",
            "12.712215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the results are not _quite_ the same, because JAX is float32 by default while NumPy is float64. float64 can be enabled before any computation is executed, but is not necessary here."
      ],
      "metadata": {
        "id": "2napj7JwJEIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX is also clever enough that it can take NumPy arrays as input and convert them to JAX arrays, and vice-versa (though this can often become confusing and lead to errors when using more advanced JAX features)"
      ],
      "metadata": {
        "id": "hYb5_UrwJzqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f_jax(x_np))\n",
        "print(f_np(x_jax))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLP845vFJ4eF",
        "outputId": "5f17aa49-4e20-48f5-c9e7-e277eafc1f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.712215\n",
            "12.712215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Immutable Arrays"
      ],
      "metadata": {
        "id": "ZXgEIsQ7Kk6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One difference from NumPy and other numerical packages is that JAX is *stateless*. Given the same inputs, a JAX function is guaranteed to produce the same outputs (modulo some nondeterminism for certain operations on GPUs). This enables JAX to analyze the structure of these functions and make program transformations that are guaranteed to be correct.\n",
        "\n",
        "This statelessness extends to arrays - in-place operations are *not* allowed on JAX arrays. Instead, there is semantic sugar for creating a new array which is identical up to the indices to be updated. While this may seem inefficient, when combined with just-in-time compilation, the compiler is clever enough to turn these into in-place operations when appropriate."
      ],
      "metadata": {
        "id": "_kKW3P2iMO0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In-place operations are fine for NumPy\n",
        "x_np[2] = 4\n",
        "print(x_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272920e9-d1c9-4d1e-8d82-174b6adc61d9",
        "id": "2H81FZ5DDV6E"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# But a new array must be created in JAX\n",
        "x_jax = x_jax.at[2].set(4)\n",
        "print(x_jax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IYcTCXnOJ8d",
        "outputId": "5c66176c-3900-487e-8ebd-453234cbb723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add, and mul replace += and *=\n",
        "x_jax = x_jax.at[2].add(2)\n",
        "print(x_jax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfgTQEFQ1kX2",
        "outputId": "ff9f82db-93aa-4aed-f57d-5495a588d850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX is also stricter about type than NumPy"
      ],
      "metadata": {
        "id": "3dJAoVaOU6lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_jax.at[2].set(3.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XeCy4AYU8vi",
        "outputId": "cbf9ba78-1654-4a77-a918-c66bc75cbcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/ops/scatter.py:90: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([1, 2, 3], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `jax.jit`: Just in time compilation"
      ],
      "metadata": {
        "id": "CmweQ1rGyp2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where JAX starts to diverge from NumPy is in its various _program transformations_. These are decorators which can be added to JAX functions which change how they behave. One of the most useful for getting good performance is `jax.jit` - JIT being short for \"just-in-time\", a type of compilation that happens _as a program is being executed_, rather than before execution (in fact JAX stands for \"Just After eXecution\", reflecting its particular compilation strategy).\n",
        "\n",
        "A function decorated with `jax.jit` will execute normally as interpreted Python the first time it is run, but as it is being run, the JAX compiler traces the function into a domain-specific language called a \"jaxpr\" (short for _JAX Expression_) which can then be handed off to the compiler for XLA, the backend used by JAX. This compiled function can then be executed efficiently (sometimes much more efficiently) on any hardware that supports XLA. Compilation is sometimes slow the first time a function is executed, but the compiled code is fast thereafter.\n",
        "\n",
        "The compiled code assumes a _fixed input size_ - if a new input of a different size is provided to the function, compilation is triggered again. The compiler also assumes a _fixed computation graph_ - if the code being compiled has any control statments like `if`, `for` or `while`, it will assume that the particular branch followed during execution is _always_ followed. If that is not the desired behavior, then there are special control statements for JAX such as `jax.lax.cond` for conditionals and `jax.lax.while_loop` for while loops. Using these can also speed up compilation and reduce the memory requirements, but they are outside the scope of this tutorial."
      ],
      "metadata": {
        "id": "A9No5afnXhOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def f_jax(x):\n",
        "  print('tracing!')\n",
        "  y = 2 * x\n",
        "  z = x + 1\n",
        "  return jnp.dot(jnp.log(y), z)"
      ],
      "metadata": {
        "id": "FXToghcmaNs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first time the code is executed, it is traced"
      ],
      "metadata": {
        "id": "SUtlbXCwb4ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_jax(x_jax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEfOczjUaPtG",
        "outputId": "d7bed3df-f9e3-452f-bb6d-8a4afc0cdb8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tracing!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(22.939524, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But not the second time"
      ],
      "metadata": {
        "id": "yK_ALi5ib-BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_jax(x_jax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fELdBC5PaxTX",
        "outputId": "bb55e3b6-c6d2-47a8-cef0-b2e75a857134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(22.939524, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unless the shape of the input is changed"
      ],
      "metadata": {
        "id": "W4Nc0Tnvb_oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_jax(jnp.ones(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nulrR8HVakck",
        "outputId": "4e5811ce-2d9c-462e-bef1-1621ac69f593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tracing!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(6.931472, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `jax.vmap` and `jax.pmap`: Autobatching and parallelism"
      ],
      "metadata": {
        "id": "_fJhBKDaK4Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once a function is traced, the real power of JAX emerges. These traced functions can be _transformed_ in all sorts of useful ways. For instance, functions which are written to act on a single data point can be automatically converted to act on batches by using the `jax.vmap` transformation. The same idea can be used to easily parallelize JAX functions using the `jax.pmap` transformation.\n",
        "\n",
        "In both cases, the transformed function expects an input with one more dimension than the function is actually written for. In the case of `pmap`, that extra dimension must have the same size as the number of devices available. We won't focus on `pmap` in this tutorial except to say that it makes multi-device training very easy to express."
      ],
      "metadata": {
        "id": "B9XJHey0cMnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# multiply an entire batch of matrices by a single matrix using vmap\n",
        "mat = np.random.randn(4, 3)\n",
        "data = np.random.randn(3, 2, 10)  # here, the *last* dimension is the batch dimension\n",
        "\n",
        "try:\n",
        "  result = jnp.matmul(mat, data)\n",
        "except:\n",
        "  print('Naively, this fails')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85b97fa-e260-4fc2-f5c5-dfe4bc23a3cb",
        "id": "3g7ugL_ADV6F"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naively, this fails\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`in_axes` specifies, for each argument, which dimension is the batch dimension, while `out_axes` specifies the batch dimension of the output (default 0)\n",
        "\n",
        "If the given input is not batched, the axis is given as None"
      ],
      "metadata": {
        "id": "ZyZWAw3w6dJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vmap_matmul = jax.vmap(jnp.matmul, in_axes=(None, -1), out_axes=-1)\n",
        "vmap_matmul(mat, data).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyA0yQnr6cSg",
        "outputId": "8083597c-c867-4cff-d3c3-77b7c941b530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 2, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`vmap` can also be used as a decorator"
      ],
      "metadata": {
        "id": "QJ6cAJ5j6nTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "@partial(jax.vmap, in_axes=(None, -1), out_axes=-1)\n",
        "def vmap_decorator_matmul(mat, data):\n",
        "  return jnp.matmul(mat, data)\n",
        "\n",
        "vmap_decorator_matmul(mat, data).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMUSif184r_p",
        "outputId": "5f6a6e12-9076-497b-9923-df0c4718937d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 2, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pmap` functions similarly to `vmap` - it changes the function such that an extra input dimension is required for *all* inputs, and that extra dimension must match the number of devices. As we will not be doing any parallel training in this tutorial, we won't go further into `pmap`, but it does make parallelization quite straightforward in most cases."
      ],
      "metadata": {
        "id": "5AsLi1637NI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.devices())\n",
        "\n",
        "pmap_matmul = jax.pmap(jnp.matmul, axis_name='batch')\n",
        "\n",
        "pmap_mat = np.random.randn(len(jax.devices()), 4, 3)\n",
        "pmap_data = np.random.randn(len(jax.devices()), 3, 2)\n",
        "pmap_matmul(pmap_mat, pmap_data).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iTcEqC27T0M",
        "outputId": "4dce09f0-099f-497d-f3d9-6849cabceddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GpuDevice(id=0, process_index=0)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `jax.grad`: Automatic differentiation"
      ],
      "metadata": {
        "id": "qz4G4a41LCJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we come to the main purpose of JAX - automatic differentiation. JAX's program transformations can take any pure-JAX function and automatically compute its derivatives in any number of ways. The one most applicable for machine learning is the `jax.grad` transformation. This takes a function with a single scalar output and returns another function which computes the gradients with respect to the inputs of the original function."
      ],
      "metadata": {
        "id": "q57OdhRKBbVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(jnp.sin(1.0))  # normal function evaluation\n",
        "\n",
        "print(jax.grad(jnp.sin)(1.0))  # evaluate the gradient\n",
        "print(jnp.cos(1.0))  # should match the analytic gradient\n",
        "\n",
        "print(jax.value_and_grad(jnp.sin)(1.0))  # returns both value and gradient, to avoid repeated computation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5383f1e4-f7cc-46cf-cf86-8644993ca104",
        "id": "RfzI0LxQDV6F"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.841471\n",
            "0.5403023\n",
            "0.5403023\n",
            "(DeviceArray(0.841471, dtype=float32, weak_type=True), DeviceArray(0.5403023, dtype=float32, weak_type=True))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`jax.grad` can be applied repeatedly to compute derivatives of any order cheaply"
      ],
      "metadata": {
        "id": "J7bqBH83Cmmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.grad(jax.grad(jnp.sin))(1.0))  # d^2 sin(x)/ dx^2 = -sin(x) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIEAQOqKC2GH",
        "outputId": "356e8975-6a32-4228-a7f4-3c45bd747746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.841471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For machine learning applications, we usually want to compute the gradient with respect to parameters and not data. For functions with multiple arguments we can use `argnums` to specify which derivatives we want to compute."
      ],
      "metadata": {
        "id": "rol_huLjDO49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_reg(params, data, label):\n",
        "  \"\"\"A simple logistic regression model.\"\"\"\n",
        "  w, b = params\n",
        "  return label @ jax.nn.softmax(w @ data + b)\n",
        "\n",
        "n, m = 10, 5  # number of inputs and outputs\n",
        "params = np.random.randn(m, n), np.random.randn(m)\n",
        "data = np.random.randn(n)\n",
        "label = np.zeros(m)\n",
        "label[0] = 1\n",
        "\n",
        "print(log_reg(params, data, label))  # just evaluate the loss\n",
        "params_grad = jax.grad(log_reg, argnums=0)(params, data, label)\n",
        "print(params_grad[0].shape, params_grad[1].shape)  # gradient should have same shape as input\n",
        "\n",
        "# take gradient with respect to data instead\n",
        "data_grad = jax.grad(log_reg, argnums=1)(params, data, label)\n",
        "print(data_grad.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZD1bDDrM88q",
        "outputId": "77081f5a-9599-48db-bac4-96c02ec59b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.004147647\n",
            "(5, 10) (5,)\n",
            "(10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `jax.jvp` and `jax.vjp`: Forward and reverse mode AD"
      ],
      "metadata": {
        "id": "5hztE2PRDJlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Under the hood, `jax.grad` uses _reverse-mode automatic differentiation_. Forward-mode and reverse-mode AD are two different computational strategies for numerically computing exact derivatives."
      ],
      "metadata": {
        "id": "qd7LAiCcGsNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward-mode differentiation computes derivatives in the same direction as normal function evaluation - a vector of tangents the same size as the inputs is passed forward though the function as it is evaluated, and the tangent vector is multiplied by the Jacobian of each subfunction as it goes.\n",
        "\n",
        "For a function $f: \\mathbb{R}^n â \\mathbb{R}^m$, input $\\vec{x} \\in \\mathbb{R}^n$ and tangent vector $\\vec{v} \\in \\mathbb{R}^n$, forward mode AD computes the Jacobian-vector product\n",
        "\n",
        "$$\n",
        "\\mathbf{J}_f(\\vec{x}) \\vec{v}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{J}_f(\\vec{x})$ is the Jacobian of $f$ at $\\vec{x}$. Hence in JAX, the shorthand for this Jacobian-vector product is `jvp`."
      ],
      "metadata": {
        "id": "AaTHLVVnKBYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_reg_pred(params, data):\n",
        "  \"\"\"Just the prediction of logistic regression, no labels.\"\"\"\n",
        "  w, b = params\n",
        "  return jax.nn.softmax(w @ data + b)\n",
        "\n",
        "n, m = 10, 5  # number of inputs and outputs\n",
        "w, b = np.random.randn(m, n), np.random.randn(m)\n",
        "data = np.random.randn(n)"
      ],
      "metadata": {
        "id": "uCv1tMT7Gegg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If we just want the Jacobian-vector product with the data, set the other tangents to zero\n",
        "w_tangent, b_tangent = np.zeros_like(w), np.zeros_like(b)\n",
        "data_tangent = np.random.randn(*(data.shape))\n",
        "\n",
        "# jvp takes the function, a tuple of function arguments, and a tuple of tangents shaped like the function arguments\n",
        "value, jvp = jax.jvp(log_reg_pred, ((w, b), data), ((w_tangent, b_tangent), data_tangent))\n",
        "print(value)  # the value of the function\n",
        "print(jvp)  # the Jacobian-vector product of the function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZPXxR1KTKvP",
        "outputId": "bdf2b1b2-3241-4884-efce-9634ebc43930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.1929546e-02 1.2650917e-02 9.5651138e-01 1.2330459e-04 1.8784817e-02]\n",
            "[ 1.0091262e-02 -7.8532062e-02  9.2359781e-03  2.1306972e-05\n",
            "  5.9183605e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reverse-mode differentiation computes derivatives in the opposite direction, and when applied to neural networks is also known as _backpropagation_. The function is first evaluated normally, and intermediate computations are cached. Then a vector of the same size as the _output_ is given, and the function execution is stepped back through in reverse order, at each step multiplying the vector by the Jacobian from the other side.\n",
        "\n",
        "This means in reverse-mode AD, when given a vector $\\vec{v}\\in\\mathbb{R}^m$, we compute a vector-Jacobian product\n",
        "\n",
        "$$\n",
        "\\vec{v}^T \\mathbf{J}_f(\\vec{x})\n",
        "$$\n",
        "\n",
        "and the JAX shorthand for this is `vjp`. Note that the calling convention for `jax.vjp` is different - rather than returning two vectors (function output and derivative), it returns the function output and _another function_, which then has to be called."
      ],
      "metadata": {
        "id": "9Uk9z4mMKKTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rather than packing all arguments into a tuple as in jvp,\n",
        "# vjp takes the function arguments unpacked after the function itself\n",
        "\n",
        "value, vjp_fun = jax.vjp(log_reg_pred, (w, b), data)  # the forward computation\n",
        "output_tangent = np.random.randn(*(value.shape))  # the input to the reverse computation\n",
        "vjp = vjp_fun(output_tangent)  # the reverse computation\n",
        "\n",
        "# Unpack the vjp result, which has the same shape as the inputs\n",
        "vjp_params, vjp_data = vjp\n",
        "vjp_w, vjp_b = vjp_params\n",
        "\n",
        "print(vjp_w.shape, vjp_b.shape, vjp_data.shape)  # the sizes match the inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df95rIm4KLKR",
        "outputId": "2af5336d-781b-47e5-fe62-916012a7e577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 10) (5,) (10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both forward and reverse mode automatic differentiation can be far more efficient than computing and storing an entire Jacobian matrix. Typically forward mode is more efficient when the number of input dimensions is much smaller than output dimensions. In machine learning, typically the number of input dimensions (parameters) is large while the number of outputs (the loss) is small (or 1), and reverse mode is more efficient.\n",
        "\n",
        "Like all JAX operators, `jvp` and `vjp` can be composed. For instance, composing `jvp` and `vjp` with `vmap` enables computation of the full Jacobian, which JAX aliases as `jacfwd` and `jacrev` respectively. Composing `jacrev` and `jacfwd` gives direct computation of the Hessian, aliased as `hessian`."
      ],
      "metadata": {
        "id": "HCUECYUQKLgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `jax.custom_jvp` and `jax.custom_vjp`: Custom gradients"
      ],
      "metadata": {
        "id": "fzn6FrwYLLke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For most machine learning applications, the exact gradients of the appropriate loss function are all you need. But there are many cases where the \"true\" gradient needs to be overridden. For instance, in variational QMC, changing the wavefunction ansatz changes the probability distribution from which points are sampled, and this introduces an additional term in the gradient of the expected energy which is not accounted for by simply taking the gradient of the average energy at a fixed set of points.\n",
        "\n",
        "JAX provides the ability to override the default gradients for any function. There are two methods for doing so: `custom_jvp` and `custom_vjp`, which as the name suggests, overrides either the forward-mode or reverse-mode gradient, respectively. `jax.grad` is a wrapper around `jax.vjp`, so often if only reverse-mode gradients are needed, it is more direct to use `custom_vjp`. However, JAX is clever enough that it can automatically convert a `custom_jvp` rule into a `custom_vjp` rule, meaning that custom reverse-mode gradients can be obtained _for free_ given forward-mode gradients. Thus if more flexibility is desired, `custom_jvp` is usually the preferred method."
      ],
      "metadata": {
        "id": "yOBuaMQlNIaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `custom_jvp`"
      ],
      "metadata": {
        "id": "JfsPo9YDlOov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To construct a `custom_jvp`, just add the appropriate decorator to the desired function, define the custom rule, and register it appropriately."
      ],
      "metadata": {
        "id": "i77UiUV6g4B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.custom_jvp\n",
        "def log_reg_2(params, data, label):\n",
        "  \"\"\"Just the prediction of logistic regression, no labels.\"\"\"\n",
        "  w, b = params\n",
        "  return label @ jax.nn.softmax(w @ data + b)\n",
        "\n",
        "@log_reg_2.defjvp\n",
        "def log_reg_2_jvp(primals, tangents):\n",
        "  # The primals are just the arguments to the function\n",
        "  params, data, labels = primals\n",
        "  w, b = params\n",
        "\n",
        "  # The tangents are the same shape as the primals\n",
        "  params_t, data_t, labels_t = tangents\n",
        "  w_t, b_t = params_t\n",
        "\n",
        "  # The forward computation must be returned at the end\n",
        "  value = log_reg_2(params, data, labels)\n",
        "\n",
        "  # To automatically derive the vjp from the jvp, the custom_jvp\n",
        "  # rule must be a linear function of the tangents (but not the primals)\n",
        "  dw = jnp.sum(w_t)\n",
        "  db = jnp.sum(b_t)\n",
        "  ddata = jnp.sum(data_t)\n",
        "  dlabels = jnp.sum(labels_t)\n",
        "\n",
        "  return value, (value ** 2) * (dw + db + ddata + dlabels)"
      ],
      "metadata": {
        "id": "Je0eROEBDV6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n, m = 10, 5  # number of inputs and outputs\n",
        "params = np.random.randn(m, n), np.random.randn(m)\n",
        "data = np.random.randn(n)\n",
        "label = np.zeros(m)\n",
        "label[0] = 1"
      ],
      "metadata": {
        "id": "PDhNFQn4jEZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the true gradient"
      ],
      "metadata": {
        "id": "WguHeA06jZXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax.value_and_grad(log_reg, argnums=1)(params, data, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdVxPBPyjVHt",
        "outputId": "78140387-5a83-4557-8908-f6b677f837c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(0.033994, dtype=float32),\n",
              " DeviceArray([ 9.3485694e-03,  8.0663711e-05, -1.7548382e-02,\n",
              "              -1.5868222e-02,  4.9521625e-02, -2.6459767e-02,\n",
              "              -3.4462489e-02,  1.6669406e-02,  6.3038804e-02,\n",
              "              -4.3170154e-02], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the custom gradient"
      ],
      "metadata": {
        "id": "fxnbhIy_ja4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax.value_and_grad(log_reg_2, argnums=1)(params, data, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1LVzispjK0W",
        "outputId": "b47af621-870d-4c8d-b745-e14a8c936f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(0.033994, dtype=float32),\n",
              " DeviceArray([0.00115559, 0.00115559, 0.00115559, 0.00115559, 0.00115559,\n",
              "              0.00115559, 0.00115559, 0.00115559, 0.00115559, 0.00115559],            dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the values are the same in both cases, because we returned the true value of the function in the `custom_jvp`. We could in theory have overridden that as well."
      ],
      "metadata": {
        "id": "h9KExlsBjdyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We mentioned above that the `custom_jvp` rule must be a linear function of the tangents (though not the primals). This is because JAX converts a `jvp` rule into a `vjp` rule by evaluating the transpose of any linear function applied to the tangents. If no such transpose rule exists, it will throw an error when evaluating the gradient (although forward-mode differentiation will still execute fine)."
      ],
      "metadata": {
        "id": "wO6p1BWaj3iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.custom_jvp\n",
        "def log_reg_2_nonlin(params, data, label):\n",
        "  \"\"\"Just the prediction of logistic regression, no labels.\"\"\"\n",
        "  w, b = params\n",
        "  return label @ jax.nn.softmax(w @ data + b)\n",
        "\n",
        "@log_reg_2_nonlin.defjvp\n",
        "def log_reg_2_nonlin_jvp(primals, tangents):\n",
        "  params, data, labels = primals\n",
        "  w, b = params\n",
        "\n",
        "  params_t, data_t, labels_t = tangents\n",
        "  w_t, b_t = params_t\n",
        "\n",
        "  value = log_reg_2_nonlin(params, data, labels)\n",
        "\n",
        "  dw = jnp.sum(w_t)\n",
        "  db = jnp.sum(b_t)\n",
        "  ddata = jnp.sum(data_t) ** 2  # <------- Note the nonlinearity!!!\n",
        "  dlabels = jnp.sum(labels_t)\n",
        "\n",
        "  return value, (value ** 2) * (dw + db + ddata + dlabels)"
      ],
      "metadata": {
        "id": "91M0DSVakXQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  jax.value_and_grad(log_reg_2_nonlin, argnums=1)(params, data, label)\n",
        "except:\n",
        "  print('Totally fails!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TfGUS_XkiZ4",
        "outputId": "8c770a07-2fec-4da5-afc7-07f419d68c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Totally fails!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `custom_vjp`"
      ],
      "metadata": {
        "id": "gj4KSqqMlSNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The structure for creating a `custom_vjp` is only slightly different. Rather than a single function which simultaneously computes the function and Jacobian-vector product, two separate functions must be created, one which executes the forward computation and provides any auxilliary information (to avoid repeated computation), the other which computes the vector-Jacobian product."
      ],
      "metadata": {
        "id": "B3ecOp1ll0-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.custom_vjp\n",
        "def log_reg_3(params, data, label):\n",
        "  \"\"\"Just the prediction of logistic regression, no labels.\"\"\"\n",
        "  w, b = params\n",
        "  return label @ jax.nn.softmax(w @ data + b)\n",
        "\n",
        "# Forward function has the same arguments as the function to override,\n",
        "# but returns residual data that can be used on the backwards pass\n",
        "def log_reg_3_fwd(params, data, label):\n",
        "  value = log_reg_3(params, data, label)\n",
        "  res = params, jnp.exp(data), label ** 2\n",
        "  return value, res\n",
        "\n",
        "# Backward function takes the residual data, as well as a vector `g` of the\n",
        "# same shape as the forward output. The return value must have the same shape\n",
        "# as the input to the forward function\n",
        "def log_reg_3_bwd(res, g):\n",
        "  w, b = res[0]\n",
        "  return (g * w, g * b), g ** 2 * res[1], g * res[2]\n",
        "\n",
        "log_reg_3.defvjp(log_reg_3_fwd, log_reg_3_bwd)"
      ],
      "metadata": {
        "id": "BgkqonWjmudW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.value_and_grad(log_reg, argnums=1)(params, data, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yD7JTRfo4Ao",
        "outputId": "157e6b32-33fc-4d2f-9537-dd30b312fe30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(0.033994, dtype=float32),\n",
              " DeviceArray([ 9.3485694e-03,  8.0663711e-05, -1.7548382e-02,\n",
              "              -1.5868222e-02,  4.9521625e-02, -2.6459767e-02,\n",
              "              -3.4462489e-02,  1.6669406e-02,  6.3038804e-02,\n",
              "              -4.3170154e-02], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jax.value_and_grad(log_reg_3, argnums=1)(params, data, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVaQ4tHnocEy",
        "outputId": "68e87664-289a-4b07-927c-b8e80da1ec97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(0.033994, dtype=float32),\n",
              " DeviceArray([0.8993844 , 0.44218853, 0.48642257, 0.1992278 , 0.21630265,\n",
              "              2.3924632 , 4.894981  , 0.5387306 , 1.0779221 , 0.6203646 ],            dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, the value is the same (because we did not override it) but the gradient is different"
      ],
      "metadata": {
        "id": "Zm5z9oeKo700"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random number generators"
      ],
      "metadata": {
        "id": "W6gDTsfcLFm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One last wrinkle with JAX. You may have noticed that whenever we needed a random number, we used NumPy rather than JAX. That's because JAX's statelessness applies to random number generators as well. To make execution as deterministic as possible, JAX makes the random number generator state explicit, meaning you have to manage the random number state if you require a source of randomness inside a compiled JAX function. Often it is enough to simply have the random numbers generated outside JAX functions and passed in, but in other cases that might be too slow, as it can involve sending large amounts of data between devices and host."
      ],
      "metadata": {
        "id": "iTWhMWarUaU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate an RNG key with a fixed seed"
      ],
      "metadata": {
        "id": "Y5gBerDpX5Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = jax.random.PRNGKey(23)\n",
        "print(key)  # the key is a tuple of two integers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0ecbee2-0c8b-4e97-e947-03eefe30bcf1",
        "id": "VR8OnIXRDV6H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0 23]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the key to sample from a random normal with unit variance and zero mean"
      ],
      "metadata": {
        "id": "IdfjqKeIX6_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax.random.normal(shape=[2, 3], key=key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHaYjqugXwvx",
        "outputId": "f570446c-9e73-42f3-811b-0887d95590dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[0.75366443, 0.24838842, 1.6752015 ],\n",
              "             [0.60685104, 0.7336006 , 0.8622222 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the same key twice will yield the same result"
      ],
      "metadata": {
        "id": "2J-vMEn2YB9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax.random.normal(shape=[2, 3], key=key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSJCgzBlYBgC",
        "outputId": "7f2c9b1d-f331-4f37-ccfd-92199806f791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[0.75366443, 0.24838842, 1.6752015 ],\n",
              "             [0.60685104, 0.7336006 , 0.8622222 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `jax.random.split` to generate a one-time use subkey and a new key for the next random computation"
      ],
      "metadata": {
        "id": "GWmZ8AvlYNxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key, subkey = jax.random.split(key)\n",
        "jax.random.normal(shape=[2, 3], key=subkey)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1IQnq4pYWBu",
        "outputId": "68c34f73-b13a-4157-f958-3f79b26bb769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[ 0.43310288, -0.7762475 ,  0.0924614 ],\n",
              "             [ 0.85738564,  0.755051  , -0.9251763 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's different!"
      ],
      "metadata": {
        "id": "KfGUbJcOZX3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Monte Carlo basics\n",
        "Now onto some physics! We will go through just enough details to implement Variational Monte Carlo in jax. \n",
        "\n",
        "## Some definitions\n",
        "Our goal in VMC is to find the best possible approximation to the solution of the SchrÃ¶dinger equation,\n",
        "\\begin{equation}\n",
        "  \\hat H|\\psi\\rangle = E |\\psi\\rangle,\n",
        "\\end{equation}\n",
        "where $\\hat H$ is the electronic Hamiltonian,\n",
        "\\begin{equation}\n",
        "  \\hat H = -\\frac{1}{2} \\nabla^2 + \\frac{1}{2} \\sum_{i\\neq j} \\frac{1}{r_{ij}} - \\sum_{iI} \\frac{Z_I}{r_{iI}} + \\frac{1}{2}\n",
        "  \\sum_{I \\neq J} \\frac{Z_I Z_J}{r_{IJ}},\n",
        "\\end{equation}\n",
        "The indices determines the particle type, such that $i,j$ label electrons, $I,J$ label atoms in the Born-Oppenheimer approximation, and $r_{pq} = |\\vec r_p - \\vec r_q|$ gives the distance between two arbitrary particles. $Z_I$ are the atomic charges. (note the atomic units $\\hbar = m_e = a_0 = e = 1$. One unit of energy is a Hartree, or roughly 27.21 eV)\n",
        "\n",
        "\n",
        "We are working with explicit representations of the wavefunction in configuration space, $\\psi(\\vec R; \\theta)$. A configuration is the $3N$ vector (for $N$ electron in 3D) of all electrons in the system, $\\vec R = \\{\\vec r_1, \\dots , \\vec r_N\\}$. $\\theta$ is a set of parameters describing the wavefunction. Given a functional form of $\\psi(\\vec R; \\theta)$, it is easy to get the derivatives with respect to the parameters and/or the electron coordinates using JAX, as we will see.\n",
        "\n",
        "As an example, we start with the hydrogen wavefunction. We already know the exact form of the hydrogen ground state, $\\psi_a(\\vec r_1) = \\exp(-r_1/a)$ (up to an irrelevant normalization constant), where with $a=1$ gives the *exact* hydrogen ground state."
      ],
      "metadata": {
        "id": "w943oXcnE35T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpHkb7OLEy2E"
      },
      "outputs": [],
      "source": [
        "def psi_h(params, r):\n",
        "  \"\"\"Evaluates the (unnormalized) wavefunction for a hydrogen atom with a single electron.\n",
        "  \n",
        "  Arguments:\n",
        "    params: tuple (a,) containing the length scale a\n",
        "    r: nd array like, shape (3,).\n",
        "  Outputs\n",
        "  \"\"\"\n",
        "  if r.shape != (3,): raise ValueError(\"positions have wrong shape, should be 3,\")\n",
        "  a = params[0]\n",
        "  return jnp.exp(-jnp.linalg.norm(r)/a)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using vmap, we evaluate and plot the wavefunction along the z axis, "
      ],
      "metadata": {
        "id": "5i684_xl_7Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "batch_psi_h = jax.vmap(psi_h, in_axes=(None, 1))\n",
        "\n",
        "params = (1,)\n",
        "n_batch = 100\n",
        "r_batch = np.zeros((3, n_batch))\n",
        "r_batch[2] = np.linspace(0,5,n_batch)\n",
        "plt.plot(r_batch[2], batch_psi_h(params, r_batch))\n",
        "plt.ylabel(\"$|\\psi(r)|^2$\")\n",
        "plt.xlabel(\"z\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "TjIiqN0PAGIT",
        "outputId": "2993beda-531d-40dd-978f-fc81bd6195f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'z')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEGCAYAAACO8lkDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRV9b338fc352QegYQkECYlzDMpWlHrLChCW6tVS6+2tix7W2/H+zy297b31t7ep70dllZtq9Y63A5OrRbrgBPOooZRZhAZwpQwBBJC5u/zxznYSAAD5Jyd5Hxea2Xl7H02yeesWj7s/fvt/TN3R0REpK2koAOIiEjXo3IQEZF2VA4iItKOykFERNpROYiISDvhoAN0lvz8fB88eHDQMUREupWFCxfucveCw/f3mHIYPHgw5eXlQccQEelWzGzTkfbrspKIiLSjchARkXZUDiIi0o7KQURE2lE5iIhIO3EvBzP7vZlVmtnyo7xvZvYrM1tvZsvMbFK8M4qIJLogzhzuA6Yd4/3pQGn0aw7wmzhkEhGRNuJeDu7+CrDnGIfMAh7wiAVAnpkVxyrPvBU7uPf192P140VEuqWuOObQH9jSZrsiuq8dM5tjZuVmVl5VVXVCv+z5lTv55XNraWxuPaE/LyLSE3XFcugwd7/L3cvcvaygoN3d3x1y8egiauqbWbBhdyenExHpvrpiOWwFBrTZLonui4kzS/NJTw7x7ModsfoVIiLdTlcsh7nAP0VnLZ0O7HP37bH6ZWnJIT4xrIDnVu6ktVVLpoqIQDBTWf8MvAkMN7MKM7vezG4wsxuihzwFbADWA3cD/xzrTBePKWTn/gaWVlTH+leJiHQLcX8qq7tf/RHvO/DVOMUB4LzhhYSSjGdX7mTiwF7x/NUiIl1SV7ysFHe5Gcmcfkpvnl2hcQcREVA5fODi0UW8V3WA9ZW1QUcREQmcyiHqgpGFAJq1JCKCyuED/fLSGVeSy7wVO4OOIiISOJVDGxePLmLplmq2VR8MOoqISKBUDm1MH1MEwNPLdWlJRBKbyqGNUwqyGFGUzdPvxuyeOxGRbkHlcJhLxxZTvmkvO/bVBx1FRCQwKofDTB8beTr4M8t19iAiiUvlcJihfbMYXpjNU+9q3EFEEpfK4Qimjy3inU17qNyvS0sikphUDkdwydhi3OEZPU5DRBKUyuEIhhVmM7RvFk9p1pKIJCiVw1FcMraYt97XpSURSUwqh6O4bFzk0tKTOnsQkQSkcjiK0sJsRhRlM3fptqCjiIjEncrhGGZO6MfizdVs2VMXdBQRkbhSORzDZeP6AejsQUQSjsrhGAb0zmDSwDyeUDmISIJROXyEmeP7sXpHDet21gQdRUQkblQOH+GSccUkmS4tiUhiUTl8hL7ZaXz81D7MXboNdw86johIXKgcOmDW+P5s2l3Hki3VQUcREYkLlUMHTBtbRGo4ib8u2hp0FBGRuFA5dEBOWjIXjirkiWXbaGxuDTqOiEjMqRw66PJJJVTXNfHSmsqgo4iIxJzKoYPOKs0nPytFl5ZEJCGoHDooHEpi5vj+vLi6kuq6xqDjiIjElMrhOHx6Un8aW1r5+zI9qVVEejaVw3EY3S+H0r5ZPLZYl5ZEpGdTORwHM+PTk0pYuGkv7+86EHQcEZGYCaQczGyama0xs/VmdtMR3h9oZvPNbLGZLTOzS4LIeSSfmtifJINHF24JOoqISMzEvRzMLATcAUwHRgFXm9moww77d+Bhd58IXAX8Or4pj64oN41zhvfl0YUVNLfongcR6ZmCOHOYAqx39w3u3gg8CMw67BgHcqKvc4Eu9dS7K8tK2Lm/gVfX7Qo6iohITARRDv2BttdkKqL72vpPYLaZVQBPATce6QeZ2RwzKzez8qqqqlhkPaLzRhTSOzOFh8t1aUlEeqauOiB9NXCfu5cAlwD/a2btsrr7Xe5e5u5lBQUFcQuXEk7iUxP78/yqneyubYjb7xURiZcgymErMKDNdkl0X1vXAw8DuPubQBqQH5d0HXRl2QCaWlzTWkWkRwqiHN4BSs1siJmlEBlwnnvYMZuB8wHMbCSRcojfdaMOGF6UzfgBeTxcvkXrPIhIjxP3cnD3ZuBrwDxgFZFZSSvM7GYzmxk97NvAl81sKfBn4Drvgn8DX1lWwtqdtVrnQUR6nHAQv9TdnyIy0Nx23w/avF4JTI13ruM1c3w/fvzkKv789mYmDuwVdBwRkU7TVQeku4XstGRmTejH3KXb2HewKeg4IiKdRuVwkq6ZMoj6plYe18C0iPQgKoeTNLYkl7H9c/nTW5s1MC0iPYbKoRN87rSBrNlZw6LNe4OOIiLSKVQOneCy8f3ISg3zxwWbg44iItIpVA6dIDM1zKcm9ufv727XKnEi0iOoHDrJNacNpLG5lUfKK4KOIiJy0lQOnWRkcQ5TBvfmgQUbaWnVwLSIdG8qh0507RmD2bLnIC+tqQw6iojISVE5dKKLRhdSlJPGfW9sDDqKiMhJUTl0ouRQErNPH8ir63bxXlVt0HFERE6YyqGTXTVlICmhJB7Q2YOIdGMqh06Wn5XKjHHFPLqwgpp6PW9JRLonlUMMXHvGYA40tmhaq4h0WyqHGBg/II/Jg3px7xvva1qriHRLKocY+fJZQ9iy5yDPrtgRdBQRkeOmcoiRC0cVMbB3Br977f2go4iIHDeVQ4yEkowvTh3Mwk179bRWEel2VA4xdEXZAHLSwtzzqs4eRKR7UTnEUGZqmGtOG8TTy7ezZU9d0HFERDpM5RBj154xiCQz7tHYg4h0IyqHGCvOTeeTE/vz4Dub2XNAaz2ISPegcoiDGz5xCvVNrdz3us4eRKR7UDnEwdC+2Vw0qpD739xEbUNz0HFERD6SyiFOvnLOqew72MSDb2udaRHp+lQOcTJxYC8+fkof7n51Aw3NLUHHERE5JpVDHH3lnFPZub+BxxZtDTqKiMgxqRzi6KzSfMb2z+XXL71Hc0tr0HFERI5K5RBHZsa/nF/K5j11PL5kW9BxRESOSuUQZxeM7Muo4hxuf3Gdzh5EpMtSOcTZobOHjbvreGKZzh5EpGsKpBzMbJqZrTGz9WZ201GOudLMVprZCjP7U7wzxtJFowoZUZTNbS+u12JAItIlxb0czCwE3AFMB0YBV5vZqMOOKQW+C0x199HAN+KdM5aSkiJnDxuqDvB3nT2ISBcUxJnDFGC9u29w90bgQWDWYcd8GbjD3fcCuHtlnDPG3LTRRQwvzObW5zX2ICJdTxDl0B/Y0ma7IrqvrWHAMDN73cwWmNm0I/0gM5tjZuVmVl5VVRWjuLGRlGR866JhbNh1gL/qvgcR6WK66oB0GCgFzgGuBu42s7zDD3L3u9y9zN3LCgoK4hzx5F00qpDxJbnc+sI63TUtIl1KEOWwFRjQZrskuq+tCmCuuze5+/vAWiJl0aOYGd++aDhbqw/y4NtbPvoPiIjESRDl8A5QamZDzCwFuAqYe9gxjxM5a8DM8olcZtoQz5DxclZpPlOG9Ob2+es52KizBxHpGuJeDu7eDHwNmAesAh529xVmdrOZzYweNg/YbWYrgfnAv7r77nhnjQcz418vHk5VTQP3v7kx6DgiIgCYe8+YZ19WVubl5eVBxzhh1937Nos27eWV/3MueRkpQccRkQRhZgvdvezw/R955mBmF5rZ3WY2Ibo9JxYBE91N00dQ09DMHfPXBx1FRKRDl5W+CPwrMNvMzgMmxDZSYhpRlMNnJpVw/xub2LKnLug4IpLgOlIONe5e7e7fAS4CPhbjTAnrWxcNwwx+8eyaoKOISILrSDk8eeiFu98EPBC7OImtODed688cwuNLtrF8676g44hIAvvIcnD3vx22fVvs4sgN55xKr4xkfvzkKnrKZAER6X7CHTnIzAZ28OdVu/v+k8iT8HLSkvnGBcP4j7kreHblTi4eXRR0JBFJQB0qB+B+wAE7xjEO3IcuO520z502kD++tYkfP7mKc4YXkBoOBR1JRBJMh8rB3c+NdRD5h3Aoie/PGMXn73mb37+2ka+cc2rQkUQkwZzQHdJmlhldl0Fi5KzSAi4Y2ZfbX1xHZU190HFEJMF0qBzMLMnMrjGzJ82sElgD7Iiu1PYzMxsa25iJ6d8uHUVjSys/e0ZTW0Ukvjp65jAfOJXI6mxF7l7i7gXAmcAC4KdmNjtGGRPWkPxMvjB1CI8srGDhpr1BxxGRBNLRcrgA+DEww90/WLbM3fe4+1/c/XLgoVgETHT/cn4phTmpfP/x5VoxTkTipkPlEF1XoRWYcaxjOi2VfCArNcz3Z4xi5fb9/GHBpqDjiEiCON4B6WVm9h9m1lVXkOuRLh1bzJlD8/nFs2upqmkIOo6IJIDj/Uu+N5HFebaZ2d/M7EdmdkUMckkbZsYPZ42mvrmF/35qVdBxRCQBHFc5uPuV7j4SGAT8EFgPnBaLYPJhpxZkMefsU3hs8VZeW7cr6Dgi0sN1dCrrh+6MdvcGd1/k7vdHn9ba7hjpfDeeV8rgPhl877F3taSoiMRUh6eymtmNhz9jycxSzOw8M7sfuLbz40lbackh/vvTY9m8p45bX1gXdBwR6cE6Wg7TgBbgT2a2LXrz2/vAOuBq4BZ3vy9GGaWNM07N58qyEu5+dQMrtumx3iISGx2dylrv7r8GtgC/Bf4LmOLug9z9y+6+OJYh5cO+d8lIemUk892/vqt7H0QkJo53ttJvgL3AecBzZvaYmWV3fiw5lryMFH44cwzLKvZx5ysbgo4jIj3Q8ZbDYOBl4AZ3nwA8SmTWksTZpeOKuWRsEbc+v441O2qCjiMiPczxlkMpkUtKq81sMZE1pT9tZuebWUGnp5NjunnWGLLSwnznkaU06fKSiHSi4y2H29x9prsPJXJp6Q9ABvA54KnODifHlp+Vyo9mjeHdrfu48+X3go4jIj1IR1eCO+QZM+sFrCby2O4RwJ/d/eudnkw65NJxxTy1vJhbX1jHOcP7MqZ/btCRRKQHON47pCcBQ4HvAK8AtwPfjkEuOQ4/mjWGXhkpfPOhJdQ36eY4ETl5Hb1DeuChL6A/UAOUA8uAfm3ez4lhVjmK3pkp/PyK8ayrrOUnT68OOo6I9AAdvax0P+DAsR6R4cB9wAMnmUlOwNnDCvjC1MHc+/pGzh3Rl08M0/wAETlxHSoHdz831kHk5P3faSN4ff0uvvPIUp75+ln0yUoNOpKIdFNal6EHSUsOcetVE9l3sIlvP7KU1lYPOpKIdFOBlIOZTTOzNWa23sxuOsZxl5uZm1lZPPN1ZyOLc/j+jFG8tKaKu1/V3dMicmLiXg5mFgLuAKYDo4CrzWzUEY7LBr4OvBXfhN3f7NMGMn1MET+bt4ZFm/cGHUdEuqEgzhymAOvdfYO7NwIPArOOcNyPgJ8C9fEM1xOYGT+5fBxFuWnc+KfFVNc1Bh1JRLqZIMqhP5Gnux5SEd33ATObBAxw9yfjGawnyU1P5vZrJlFZU883H1qi8QcROS5dbkDazJKAX9KBm+vMbI6ZlZtZeVVVVezDdTMTBuTxgxmjmL+mitteXB90HBHpRoIoh63AgDbbJdF9h2QDY4CXzGwjcDow90iD0u5+l7uXuXtZQYHm9R/J7NMH8emJ/bnlhbW8tKYy6Dgi0k0EUQ7vAKVmNsTMUoCrgLmH3nT3fe6e7+6D3X0wsACY6e7lAWTt9syMH39qLMMLs/nGQ0vYsqcu6Egi0g3EvRzcvRn4GjAPWAU87O4rzOxmM5sZ7zyJID0lxG9nT6a11fnyA+UcaGgOOpKIdHHm3jMGKsvKyry8XCcXx/Lquiqu/f3bXDiqkN98bjJJScd6GoqIJAIzW+ju7S7bd7kBaYmds0oL+LdLRzFvxU5ueWFd0HFEpAs73vUcpJv74tTBrN6+n1+9sI7SvllcNr5f0JFEpAvSmUOCMTP+61Nj+NjgXnz7kaUs3LQn6Egi0gWpHBJQajjEnZ8vo19uGl9+YCGbdh8IOpKIdDEqhwTVOzOFe78whVZ3vnDvO3rEhoh8iMohgQ3Jz+Suz5dRsfcg199fzsFGLTEqIhEqhwQ3ZUhvbrlqAos27+XGPy+iuaU16Egi0gWoHIRLxhZz86wxPL+qku899i495d4XETlxmsoqAHz+9EFU1TTwqxfW0SszhZumjcBMN8mJJCqVg3zgmxeUsudAA3e+vIGslDA3nl8adCQRCYjKQT5gZtw8cwx1DS384rm1pKeE+NJZpwQdS0QCoHKQD0lKMv7nM+M42NTCfz25irTkELNPHxR0LBGJMw1ISzvhUBK3XjWR80b05d8fX84fFmwKOpKIxJnKQY4oJZzEb2ZPUkGIJCiVgxxVajjEb2ZP4vxoQfzvmxuDjiQicaJykGNKDYf49exJXDCykO//bQV3vvxe0JFEJA5UDvKRDp1BzBhXzP97ejW/eHaNbpQT6eE0W0k6JDk6SJ2ZEua2F9dTU9/MD2aM0mpyIj2UykE6LJRk/OTysWSlhbnntffZVdvAL64cT2o4FHQ0EelkKgc5LmbGv186koLsVH7y9Gr21jXy29mTyU5LDjqaiHQijTnIcTMzbvjEqfziivEs2LCHK+9cwPZ9B4OOJSKdSOUgJ+zyySX8/rqPsWVPHZ+843WWb90XdCQR6SQqBzkpnxhWwKNf+TghM668801eWLUz6Egi0glUDnLSRhTl8PhXpzK0bxZfeqCc37z0nqa6inRzKgfpFH1z0nhozse5dGwxP31mNV9/cImWHRXpxjRbSTpNekqI266eyMjiHH7+7Breq6rlt7MnM6B3RtDRROQ46cxBOpWZ8dVzh3LPtWVs3lPHjNteY/6ayqBjichxUjlITJw3opC/33gm/fLS+eJ97/DL59bS0qpxCJHuQuUgMTOoTyaP/fMZXD6phF+9sI7P/W4BO/fXBx1LRDpA5SAxlZYc4udXjOfnV4xn6ZZ9TL/1Veav1mUmka5O5SBx8ZnJJTxx45n0zU7lC/e9w3/8bTn1TZrNJNJVBVIOZjbNzNaY2Xozu+kI73/LzFaa2TIze8HMtIhxDzC0bxaPf3UqX5w6hPvf3MSM217TXdUiXVTcy8HMQsAdwHRgFHC1mY067LDFQJm7jwMeBf4nviklVtKSQ/zgslH84frTqKlv4pN3vM4tz6+lqaU16Ggi0kYQZw5TgPXuvsHdG4EHgVltD3D3+e5eF91cAJTEOaPE2Jml+cz7xtnMGFfMLc+vY9btr7Ny2/6gY4lIVBDl0B/Y0ma7IrrvaK4Hnj7SG2Y2x8zKzay8qqqqEyNKPORlpHDLVRO58/OTqaxpYObtr/HzeWs0FiHSBXTpAWkzmw2UAT870vvufpe7l7l7WUFBQXzDSae5eHQRz33zbGZO6Mft89cz/dZXeeO9XUHHEkloQZTDVmBAm+2S6L4PMbMLgH8DZrp7Q5yySUB6Zabwyysn8McvnUarO9fc/RbfemgJlTW6L0IkCEGUwztAqZkNMbMU4CpgbtsDzGwicCeRYtCk+AQydWhkLOJr5w7l78u2c/7PX+be19+nWQPWInEV93Jw92bga8A8YBXwsLuvMLObzWxm9LCfAVnAI2a2xMzmHuXHSQ+UlhziOxcP55lvnMWEgXn88ImVTL/1VV5Zq3ElkXixnvLc/bKyMi8vLw86hnQyd2feip3891Or2LynjvNH9OW7l4xkaN+soKOJ9AhmttDdyw7f36UHpEXMjGljinjuW2fz3ekjeOv9PVx8yyt877F3qdRzmkRiRmcO0q3srm3gthfX84cFm0gOJfHFMwcz56xTyc1IDjqaSLd0tDMHlYN0Sxt3HeBnz67hyWXbyUkLM+fsU7hu6hCyUrV+lcjxUDlIj7Ry235++dwanl9VSV5GMl86cwj/dMZgctJ0JiHSESoH6dGWbKnmVy+s48XVleSkhblu6hCuO2MwvTNTgo4m0qWpHCQhvFuxj1+9uI7nVu4kPTnEZz82gC+dNYSSXlrHWuRIVA6SUNbtrOG3L2/gb0u24sD0MUV86axTmDAgL+hoIl2KykES0rbqg9z/xkb+9PZmauqbmTQwj2vPGMz0McWkhDWTW0TlIAmttqGZR8q3cP8bG9m4u478rFSumTKAz04ZSP+89KDjiQRG5SACtLY6r6yr4v43NvLS2ioMOGd4X66eMpBzhxcQDulsQhKLykHkMFv21PHQO1t4qHwLVTUNFGSn8ulJ/bli8gA9nkMShspB5CiaWlqZv7qSh8srmL+mkpZWZ3xJLp+a2J/LxvejT1Zq0BFFYkblINIBlTX1/G3xNv66eCurtu8nnGScWZrPZeP6cdHoQrJ1c530MCoHkeO0esd+Hl+8jSeWbmNr9UFSwkmcXVrAJWOLOH9kIbnpKgrp/lQOIifI3Vm0uZonlm7jmeU72LG/nuSQ8fFT87loVCEXjiqkMCct6JgiJ0TlINIJWludJRXVPLN8B/NW7GDT7joAxpXkct6Ivpw/opDR/XJISrKAk4p0jMpBpJO5O+sqa3l2xQ5eXF3J4i3VuEN+VipnD8vnnOF9OXNovp7vJF2aykEkxnbXNvDSmipeXlvFK+uqqK5rAmB0vxzOLM1n6qn5lA3uRUaKHisuXYfKQSSOWlqdZRXVvLZuF6+u38XizXtpanGSQ8aEAXmcfkofpgzpzaSBvcjUGhQSIJWDSIAONDRTvmkvb763mzff28XybftpaXVCScaYfjlMHtSbssG9mDyolwa3Ja5UDiJdSG1DM4s27eWt93fzzsa9LN1STUNzKwD989KZMDCPiQPyGFeSx5j+OboUJTFztHLQf3EiAchKDXP2sALOHlYAQGNzKyu27WPR5moWb97L4s3VPLlsOwBJBsMKsxndL5ex/XMYW5LLiKIcXY6SmNJ/XSJdQEo4iYkDezFxYC9gCABVNQ0sq6hmacU+llVU8/LaSv6yqAIAMxjSJ5ORxTmMKMpmRPR7/7x0TaOVTqFyEOmiCrJTOX9kIeePLAQiU2d37K9n+db9rNq+n5Xb9vPu1n08+e72D/5MZkqIoYXZDOubRWlhFqV9sxnaN0ulIcdN5SDSTZgZxbnpFOemc+Gowg/21zY0s2ZHDat37GfdzlrW7qxh/ppKHllY8cExqeEkhuRncmpBFkPyMxmcn8mQ/AwG98mkd2YKZioO+TCVg0g3l5UaZvKgyEyntvYeaGR9VS3rdtayoaqWDbsOsGLbPp5ZsYOW1n9MRMlODTMoP4OBvTMY0Dv6vVcGJb3S6d8rndRwKN4fSboAlYNID9UrM4WPZfbmY4N7f2h/Y3MrFXvreH/XATburmPz7sj31dtreH5lJY0trR86vm92Kv17pdMvL53+eekU56ZRnJtOv7w0inLSyM9K1SWrHkjlIJJgUsJJnFKQxSkF7Rc0aml1du6vp2LvQbbsqWPL3jq2VR9ka/VBVmzdx3Mrd9LY/OHyCCcZfbNTKcxNozA7jcKcVPrmpFGQnUrf7FQKol99MlMJqUS6DZWDiHwglGT0y4ucJUwZ0rvd++7OngONbKuuZ/u+g+zcX8/2ffXs2F9P5f4G3quq5fX3dlFT39zuz5pB74wU8rNS6ZOVQu/MyOvemZHXfTJT6BV9nZeRTK+MFJK1bGtgVA4i0mFmRp+sVPpkpTK2JPeox9U3tVC5v4HKmnp21TZQVdNAVW0ju2ob2FXTwK7aBpZv3cfuA41HLJJDslPD5EaLIi8jmZz0ZPLSk8lNj7zOTU8mJy2ZnPQwOWnJZKeFyY5+T0vWWMnJUDmISKdLSw4xsE8GA/tkfOSxDc0tVNc1sedAI3sONLK3rpG9BxrZc6CJ6oONVNc1sbeukX0Hm9hafZB9dU3sO9hEc+uxn+6QEkoiOy1MVlqYrNQ2X2lhMqOvM1PCZKaGyIh+z0wJk5ESIiM18j09ORTZTgmTlpyUULO6AikHM5sG3AqEgN+5+08Oez8VeACYDOwGPuvuG+OdU0RiLzUcojAndFzPlHJ3Dja1sO9gE/sPNrO/vol9dU3UNDRRU9/c5quJ2obI69qGZrbvq6e2spm6xsh2fVPrR/+yNg6VRVpyiPSUEGnJSaSFI69Tw9Ht5H/sT0sOkRqO7EtNTiI1nERKOInUcOhDr1PCSaSEDm1HvieHDn03UkLxL6a4l4OZhYA7gAuBCuAdM5vr7ivbHHY9sNfdh5rZVcBPgc/GO6uIdE1mRkZKmIyUMMVHv7r1kVpanbrGZg40tFDb0MzBxpbIdmMzdY0t1DW2RPe1cLCphYONkUKJvG6hvimyv7ahmV21jTREtxuaW6lvirz/ESc4HXaoJJIPFUcoiXDISA4lcfs1ExlRlNM5vygqiDOHKcB6d98AYGYPArOAtuUwC/jP6OtHgdvNzLynPCVQRLqEUJJFxyhisx64u9Pc6h+URWNz64deN7a00tDUSmPLP95raG6lqaWVpuj7TS2RP9/Y3EpzS+S9Q/ubotvpMRhfCaIc+gNb2mxXAKcd7Rh3bzazfUAfYFfbg8xsDjAHYODAgbHKKyJyQsyM5Oi/7rO62YMSu/U8MXe/y93L3L2soKAg6DgiIj1GEOWwFRjQZrskuu+Ix5hZGMglMjAtIiJxEEQ5vAOUmtkQM0sBrgLmHnbMXODa6OvPAC9qvEFEJH7ifhEsOobwNWAekamsv3f3FWZ2M1Du7nOBe4D/NbP1wB4iBSIiInESyAiJuz8FPHXYvh+0eV0PXBHvXCIiEtGtB6RFRCQ2VA4iItKOykFERNqxnjIJyMyqgE0n+MfzOewGuwSgz5wY9Jl7vpP9vIPcvd2NYj2mHE6GmZW7e1nQOeJJnzkx6DP3fLH6vLqsJCIi7agcRESkHZVDxF1BBwiAPnNi0Gfu+WLyeTXmICIi7ejMQURE2lE5iIhIOwlfDmY2zczWmNl6M7sp6DyxZma/N7NKM1sedJZ4MLMBZjbfzFaa2Qoz+3rQmWLNzNLM7G0zWxr9zD8MOlO8mFnIzBab2d+DzhIPZrbRzN41syVmVt6pPzuRxxyi61mvpc161sDVh61n3aOY2dlALfCAu48JOk+smVkxUOzui8wsG1gIfLKH/29sQKa715pZMvAa8HV3XxBwtJgzs28BZUCOu88IOk+smTl814cAAAJeSURBVNlGoMzdO/2mv0Q/c/hgPWt3bwQOrWfdY7n7K0Qeg54Q3H27uy+Kvq4BVhFZhrbH8oja6GZy9KvH/yvQzEqAS4HfBZ2lJ0j0cjjSetY9+i+ORGZmg4GJwFvBJom96OWVJUAl8Jy79/jPDNwC/B+gNeggceTAs2a20MzmdOYPTvRykARhZlnAX4BvuPv+oPPEmru3uPsEIsvwTjGzHn0J0cxmAJXuvjDoLHF2prtPAqYDX41eNu4UiV4OHVnPWrq56HX3vwB/dPe/Bp0nnty9GpgPTAs6S4xNBWZGr8E/CJxnZn8INlLsufvW6PdK4DEil8o7RaKXQ0fWs5ZuLDo4ew+wyt1/GXSeeDCzAjPLi75OJzLhYnWwqWLL3b/r7iXuPpjI/49fdPfZAceKKTPLjE6ywMwygYuATpuFmNDl4O7NwKH1rFcBD7v7imBTxZaZ/Rl4ExhuZhVmdn3QmWJsKvB5Iv+SXBL9uiToUDFWDMw3s2VE/gH0nLsnxNTOBFMIvGZmS4G3gSfd/ZnO+uEJPZVVRESOLKHPHERE5MhUDiIi0o7KQURE2lE5iIhIOyoHERFpR+UgIiLtqBxERKQdlYNIjJjZDW1uvHvfzOYHnUmko3QTnEiMRZ/t9CLwP+7+RNB5RDpCZw4isXcrkWf9qBik2wgHHUCkJzOz64BBRJ7hJdJt6LKSSIyY2WTgfuAsd98bdB6R46HLSiKx8zWgN5EnpC4xMy1fKd2GzhxERKQdnTmIiEg7KgcREWlH5SAiIu2oHEREpB2Vg4iItKNyEBGRdlQOIiLSzv8HNGjCp6B/YzkAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Expectation values of observables\n",
        "To understand the system, we need to evaluate expectation values, not just values at single points.\n",
        "\n",
        "The Monte Carlo part of the name Variational Monte Carlo refers to how expecation values of operators are evaluated. Expectation values in configuration space are 3N-dimensional integrals, which suffer massively from the curse of dimensionality. Using a grid or quadrature method to evaluate the integrals scales with $d^{3N}$, where $d$ is the number of grid points per dimension. This quickly gets out of hand!\n",
        "\n",
        "Instead, we use markov chain monte carlo sampling to evaluate the integrals, here exemplified with the energy expectation value,\n",
        "\\begin{align}\n",
        "  \\langle E\\rangle_\\theta\n",
        "  &= \\frac{\\int \\mathrm d \\vec R\\, \\psi^*(\\vec R; \\theta)\\hat H \\psi(\\vec R; \\theta) } {\\int\n",
        "  \\mathrm d \\vec R\\,|\\psi(\\vec R; \\theta)|^2}\n",
        "  = \\int \\mathrm d \\vec R\\, p(\\vec R; \\theta) E_L(\\vec R; \\theta) \n",
        "  \\\\&\\approx\n",
        "   \\frac{1}{M} \\sum_i E_L(\\vec R^{(i)}; \\theta).\n",
        "\\end{align}\n",
        "We have defined the local energy as,\n",
        "\\begin{equation}\n",
        "  E_L(\\vec R; \\theta) = \\frac{\\hat H\\psi(\\vec R; \\theta)}{\\psi(\\vec\n",
        "  R; \\theta)},\n",
        "\\end{equation}\n",
        "and the integral is approximated as a sum over configurations\n",
        "$\\{\\vec R^{(i)}\\}$ that are sampled from the well-known probability distribution\n",
        "\\begin{equation}\n",
        "  p_\\theta(\\vec R^{(i)})\n",
        "  = \\frac{|\\psi(\\vec R^{(i)}; \\theta)|^2 }{ \\int \\mathrm d \\vec R\\,|\\psi(\\vec R; \\theta)|^2}\n",
        "\\end{equation}\n",
        "The local energy is an important quantity, so we stop here for a moment to implement it.\n",
        "\n",
        "\n",
        "## Problem 1\n",
        "---\n",
        "a) Implement a factory function which returns the local energy $E_L(\\theta, \\vec R)$ for a single electron in a hydrogen potential at the origin. We already supply the kinetic energy, but you will have to implement the potential energy. \n",
        "\n",
        "b) Given the hydrogen wavefunction above, evaluate the local energy at a random position. "
      ],
      "metadata": {
        "id": "FUWHCRxM8L0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_local_kinetic_energy(f):\n",
        "  f_lapl = jax.hessian(f, argnums=1)\n",
        "  def local_kinetic_energy(params, pos):\n",
        "    return - 0.5 * jnp.trace(f_lapl(params, pos)) / f(params, pos)\n",
        "  return local_kinetic_energy\n",
        "\n",
        "\n",
        "def make_local_energy(\n",
        "  f,\n",
        "  charge=1.0,\n",
        "):\n",
        "  \"\"\"Returns a function which evaluates the local energy.\n",
        "  \n",
        "  Simple version: consider just a single atom located at the (arbitrary) origin.\n",
        "\n",
        "  Args:\n",
        "    f: function with signature f(params, pos) which evaluates the wavefunction.\n",
        "    charge: charge of atom located at the origin.\n",
        "  \"\"\"\n",
        "  local_kinetic_energy = make_local_kinetic_energy(f)\n",
        "  \n",
        "  def local_energy(params, pos):\n",
        "    # implement here\n",
        "    pass \n",
        "  return local_energy\n",
        "\n",
        "\n",
        "pos = np.random.random((3,))\n",
        "params = (1.0,)\n",
        "\n",
        "local_energy = make_local_energy(psi_h, charge=1.0)\n",
        "local_energy(params, pos)"
      ],
      "metadata": {
        "id": "Q4yOQT1oFABw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f572c2fa-86c1-400a-bfe1-42ca1f401b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(-0.49999997, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should have got the exact eigenvalue for the hydrogen atom, $-0.5E_h$. The variance in the local energy is zero when the wavefunction is an eigenfunction $\\psi_n$ of the Hamiltonian with energy $E_n$, since we get $E_L = (1/\\psi_n) \\hat H \\psi_n = E_n$ independent of configuration $\\vec R$. In general, the variance of the local energy is a measure of how far we are from the exact ground state."
      ],
      "metadata": {
        "id": "0LDfDp7_I9sR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metropolis-Hastings\n",
        "\n",
        "The next step is to sample $p(\\vec R; \\theta)$ using the Metropolis-Hastings algorithm, which you might or might not know, but it is a very simple algorithm so we'll present what you need here. It works by proposing moves $\\vec R'$ locally from a 3N Gaussian distribution $N(\\vec R, \\sigma)$ with mean $\\vec R$ and standard deviation $\\sigma$. $\\sigma$ is also called the step size. (The move proposal need not be Gaussian or even symmetric, but this choice works well for many cases.)\n",
        "\n",
        "The probability of accepting a proposed move $\\vec R \\to \\vec R'$ in the Metropolis algorithm (symmetric move proposal) is given by\n",
        "\\begin{equation}\n",
        "  p(\\vec R \\to \\vec R') = \\text{min}\\left(1, \\frac{p(\\vec R')}{p(\\vec R)}\\right) = \\text{min}\\left(1, \\frac{|\\psi(\\vec R')|^2}{|\\psi(\\vec R)|^2}\\right)\n",
        "\\end{equation}\n",
        "If the wavefunction at the proposed configuration is larger than at the original position, then the proposal is always accepted. If not, then the acceptance is proportional to the ratio of the wavefunction. \n",
        "If you want more information on the Metropolis algorithm, the [Wikipedia article](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) is very good.\n",
        "\n",
        "Observe that the wavefunction doesn't have to be normalized, since the normalization factor cancels out in the ratio. This is extremely useful!"
      ],
      "metadata": {
        "id": "xcQLB1hwDNiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Important_: with local Metropolis steps, subsequent samples are highly correlated. Luckily, for this kind of VMC sampling is a lot cheaper than evaluating energies. We deal with the correlation problem by performing many configuration updates between each calculation of the observables. \n",
        "\n",
        "\n",
        "## Problem 2a\n",
        "---\n",
        "i) Implement Metropolis to equilibrate and produce a set of samples drawn from $|\\psi|^2$. Fill in the factory function below.\n",
        "\n",
        "ii) Reduce autocorrelation by running a for loop of $N_{substeps} = 50$ every time you perform Metropolis sampling.\n",
        "\n",
        "<!-- Advanced: Add the for loop inside `metropolis(...)` to handle autocorrelation. $N_{substeps}$ should be supplied to `make_metropolis`. Note that jitting a pure python loop unrolls it, which can cause long compile times scaling with the number of substeps. This is bad, and you'll quickly run into problems.\n",
        "\n",
        "Instead, do not use a regular python for loop, but a body_function (the body of the loop) and `jax.lax.fori_loop` [reference](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.fori_loop.html). Attempt at your own risk! -->"
      ],
      "metadata": {
        "id": "qFzrxm-aVOk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import functools\n",
        "from jax import lax\n",
        "from typing import Callable, List\n",
        "\n",
        "\n",
        "def make_metropolis_problem(wavefunction: Callable) -> Callable:\n",
        "  \"\"\"Constructs a function which iterates the positions of MCMC walkers according\n",
        "  to the Metropolis algorithm.\n",
        "\n",
        "  Args:\n",
        "      wavefunction: function of (params, pos)\n",
        "  Returns:\n",
        "      metropolis: function of (batch_pos, batch_prob, params, step_size, batch_keys).\n",
        "  \"\"\"\n",
        "\n",
        "  # vmap over batch dimension.\n",
        "  @functools.partial(jax.vmap, in_axes=(0,0,None,None,0))\n",
        "  def metropolis(pos, prob, params, step_size, key):  \n",
        "    \"\"\"Performs a Metropolis step.\n",
        "\n",
        "    Args:\n",
        "      pos: nd array, shape (3), current electron configurations, \n",
        "      prob: float, (unnormalized) probability of current electron configurations\n",
        "      params: pytree with wavefunction parameters\n",
        "      step_size: float\n",
        "      key: jax PRNG key. shape (batch size, *key_shape)\n",
        "    Returns:\n",
        "      batch_pos, batch_prob: updated values\n",
        "      batch_accept_count: number of updates performed per walker\n",
        "    \"\"\"    \n",
        "    accept_count = 0\n",
        "    # implement here\n",
        "    return pos, prob, accept_count\n",
        "\n",
        "  return metropolis\n"
      ],
      "metadata": {
        "id": "1utsEj9cks_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2b:\n",
        "---\n",
        "Start with an initial distribution drawn from a Gaussian from the code below. Produce a set of samples from `psi_h` that are drawn with the desired probability distribution. Verify your sampling algorithm by histogramming $r = |\\vec r|$ and comparing to the the radial distribution function supplied below. It can be illuminating to compare the histograms from the initial and final distributions, see the code below for an example of how to.\n",
        "\n",
        "Hint: whilst not critical in this case, a good rule of thumb is to aim for an acceptance rate of around 50%."
      ],
      "metadata": {
        "id": "lZEfmAA0FBhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_electrons_gaussian(key, batch_size: int,  nelec: int, init_width: float=0.5):\n",
        "  \"\"\"Initialises electron positions by sampling from a Gaussian distribution.\n",
        "\n",
        "  Args:\n",
        "    key: JAX PRNG state.\n",
        "    batch_size: number of walkers to create.\n",
        "    nelec: number of electrons in the system.\n",
        "    init_width: scale of initial Gaussian distribution.\n",
        "  \"\"\" \n",
        "  ndim = 3\n",
        "  init_pos = init_width * jax.random.normal(\n",
        "    subkey, shape=(batch_size, nelec * ndim)\n",
        "  )\n",
        "  return init_pos\n",
        "\n",
        "@jax.jit\n",
        "def batch_split(key):\n",
        "  \"\"\"Like jax.random.split, but returns one subkey per batch element.\n",
        "  \n",
        "  Returns: \n",
        "    key: jax.PRNGkey, shape (2,)\n",
        "    batch_keys: jax.PRNGkeys, shape (batch_size, 2)\n",
        "  \"\"\"\n",
        "  key, *batch_keys = jax.random.split(key, num=batch_size+1)\n",
        "  return key, jnp.asarray(batch_keys)\n"
      ],
      "metadata": {
        "id": "E-LD6EmhLt53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### plot distributions"
      ],
      "metadata": {
        "id": "VYHyVJsSGkZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histograms\n",
        "r_bins = np.linspace(0, 6.5, 100)\n",
        "\n",
        "def plot_h_rdf(params, r_bins):\n",
        "  \"\"\"Plots the radial distribution function for the hydrogen wavefunction\"\"\"\n",
        "  # array of positions along the z axis\n",
        "  pos = jnp.zeros((r_bins.size, 3)).at[:,2].set(r_bins)\n",
        "  prob = jax.vmap(psi_h, (None, 0))(params, pos)**2\n",
        "  # normalization constant and volume factor is (4/a**3) [remember psi_h is\n",
        "  # unnormalised but the rdf expects normalised wavefunctions]\n",
        "  rdf = (4/params[0]**3) * r_bins**2 * prob\n",
        "  plt.plot(r_bins, rdf, label='RDF', c='k')\n",
        "\n",
        "def hist_radial_distribution(pos, t=None):\n",
        "  r = np.linalg.norm(pos, axis=-1).ravel()\n",
        "  plt.hist(r, bins=r_bins, histtype='step', density=True, label=f't={t}');\n",
        "\n",
        "plot_h_rdf(params, r_bins)\n",
        "hist_radial_distribution(init_pos, t=0)\n",
        "hist_radial_distribution(pos, t='final')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"radial distance $r$\")\n",
        "plt.ylabel(\"wavefunction\");"
      ],
      "metadata": {
        "id": "QNwvwXtgGi_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now move on to two-electron systems for a slightly more challenging problem. Delete the old one-electron functions just to be safe."
      ],
      "metadata": {
        "id": "ksa8vVxRhhg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure to get rid of everything related to hydrogen to avoid strange hybrid calculations\n",
        "del local_energy, pos, prob, metropolis, step_size, batch_size, params, mcmc_steps, pmove"
      ],
      "metadata": {
        "id": "Uw91TgBLfB9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3\n",
        "---\n",
        "Modify the energy evaluation code to handle arbitrary number of electrons, where the electron positions are supplied as a 3N array.\n",
        "\n",
        "Hint 1: use `jnp.reshape(pos, (-1, 3))` to obtain an array of $N\\times3$ and then apply operations along the first axis, corresponding to electron indices.\n",
        "\n",
        "Hint 2: broadcasting in JAX works similarly to broadcasting in numpy."
      ],
      "metadata": {
        "id": "9NQSUfkeRhmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4\n",
        "---\n",
        "Similarly modify the Metropolis-Hastings code to handle an arbitrary number of electrons. You can either attempt a single $N$ electron move or $N$ single-electron moves, each with their own accept-reject step. For small systems, this choice doesn't have a strong impact on efficiency ([Lee *et al.*](https://doi.org/10.1103/PhysRevE.83.066706)).\n"
      ],
      "metadata": {
        "id": "viQ2ML8JRzbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient of the energy\n",
        "\n",
        "We can efficiently compute the gradient of the energy using:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta\\mathcal{L} = 2 \\mathbb{E}_{p(\\vec R)}\\left[\\left(E_L - \\mathbb{E}_{p(\\vec R)}[E_L]\\right)\\nabla_\\theta \\log|\\Psi_\\theta|\\right]\n",
        "$$\n",
        "\n",
        "In particular, this means we don't have to compute derivatives of the Hamiltonian, and so do not require third derivatives of the wavefunction! Code for doing so (adapted from the [FermiNet codebase](https://github.com/deepmind/ferminet)) is below."
      ],
      "metadata": {
        "id": "_0Z30rPBlVy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loss(network, local_energy):\n",
        "  \"\"\"Copied from Ferminet.\n",
        "\n",
        "  Creates the loss function, including custom gradients.\n",
        "\n",
        "  Args:\n",
        "  network: function, signature (params, data), which evaluates wavefunction\n",
        "    at a single MCMC configuration given the network parameters.\n",
        "  Returns:\n",
        "    total_energy: function which evaluates the both the loss as the mean of the \n",
        "      local_energies, and the local energies. With custom gradients, accounting \n",
        "      for derivatives in the probability distribution.\n",
        "  \"\"\"\n",
        "  batch_local_energy = jax.vmap(local_energy, in_axes=(None, 0), out_axes=0)\n",
        "  # FermiNet is defined in terms of the logarithm of the network, which is \n",
        "  # better for numerical stability and also makes some expressions neater.\n",
        "  log_network = lambda params, pos: jnp.log(jnp.abs(network(params, pos)))\n",
        "  batch_network = jax.vmap(log_network, in_axes=(None, 0), out_axes=0)\n",
        "\n",
        "  @jax.custom_jvp\n",
        "  def total_energy(params, data):\n",
        "    e_l = batch_local_energy(params, data)\n",
        "    loss = jnp.mean(e_l)\n",
        "    return loss, e_l\n",
        "\n",
        "  @total_energy.defjvp\n",
        "  def total_energy_jvp(primals, tangents):\n",
        "    \"\"\"Custom Jacobian-vector product for unbiased local energy gradients.\"\"\"\n",
        "    params, data = primals\n",
        "    loss, local_energy = total_energy(params, data)\n",
        "    diff = local_energy - loss\n",
        "\n",
        "    # We will walk through this section in the practical session\n",
        "    psi_primal, psi_tangent = jax.jvp(batch_network, primals, tangents)\n",
        "    primals_out = loss, local_energy\n",
        "    batch_size = jnp.shape(local_energy)[0]\n",
        "    tangents_out = (jnp.dot(psi_tangent, diff) / batch_size, local_energy)\n",
        "    return primals_out, tangents_out\n",
        "  return total_energy"
      ],
      "metadata": {
        "id": "j5B5zFlcXHe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 5\n",
        "---\n",
        "Given the 4 parameter helium wavefunction (in log space), equilibrate MCMC samples, evaluate the energy and gradient.\n",
        "\n",
        "A simple [four-parameter helium wavefunction](https://opencommons.uconn.edu/chem_educ/30/) is:\n",
        "\\begin{equation}\n",
        " \\psi_{He}(\\vec r_1, \\vec r_2; \\theta:=\\{a, b, c, d\\}) = e^{-2s}(1+\\frac{u}{2}e^{au})(1 + bsu + ct^2 + du^2) \n",
        "\\end{equation}\n",
        "where $s=r_1 + r_2$, $u=r_1 - r_2$, and $t=|\\vec r_1 - \\vec r_2|$.\n",
        "The wavefunction is defined in JAX below.\n",
        "\n",
        "Equilibrate MCMC samples, and hence estimate the energy and gradient of the energy. Compare to the exact ground state energy of helium (-2.903724377 Ha).\n"
      ],
      "metadata": {
        "id": "v92UFaItR5vE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def psi_he_hylleraas(params, pos):\n",
        "  \"\"\"Evaluates a two-electron wavefunction with four parameters based on the \n",
        "  Hylleraas parametrization of the helium atom.\n",
        "\n",
        "  C.W. David found parameters a=-1.013, b=0.2119, c=0.1409, d=-0.003 via\n",
        "  independent line searches and hence obtained an energy of -2.901188.\n",
        "  \n",
        "  Arguments:\n",
        "    params: tuple (a,) containing the length scale a\n",
        "    r: nd array like, shape (3,).\n",
        "  Outputs\n",
        "  \"\"\"\n",
        "  if pos.shape != (6,): raise ValueError(\"positions have wrong shape, should be 3,\")\n",
        "  a, b, c, d = params\n",
        "  r1, r2 = jnp.linalg.norm(pos.reshape(2, 3), axis=1)\n",
        "  r12 = jnp.linalg.norm(pos[:3] - pos[3:])\n",
        "\n",
        "  s = r1 + r2\n",
        "  t = r1 - r2\n",
        "  u = r12\n",
        "\n",
        "  f = jnp.exp(-2 * s)\n",
        "  g = 1 + b * s * u + c * t ** 2 + d * u ** 2\n",
        "  h = 1 + 0.5 * u * jnp.exp(a * u)\n",
        "  return f * g * h\n",
        "\n",
        "\n",
        "def init_hylleraas_params():\n",
        "  \"\"\"Returns an arbitrary (i.e. quite bad) set of initial parameters for the \n",
        "  hylleraas wavefunction.\"\"\"\n",
        "  return (1.0,1.0,1.0,1.0)\n"
      ],
      "metadata": {
        "id": "IukQsRrJFulb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updating parameters\n",
        "\n",
        "For small and simple systems, a simple first order optimization method such as ADAM is sufficient. The [`optax`](https://github.com/deepmind/optax) library provides many optimizers and useful utilities for optimization.\n",
        "\n",
        "We can create and initialise the optimizer with the following code:"
      ],
      "metadata": {
        "id": "rY446GnrSbQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "\n",
        "params = init_hylleraas_params()\n",
        "init_learning_rate = 2e-2\n",
        "optimizer = optax.adam(init_learning_rate)\n",
        "opt_state = optimizer.init(params)"
      ],
      "metadata": {
        "id": "fdNupnfVFUvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other optimizers in `optax` share the same API: the `init` method initialises the optimizer and takes an arbitary structure (a \"PyTree\") of trainable parameters. This conveniently enables parameters to be organised as nested dictionaries, for example."
      ],
      "metadata": {
        "id": "keh1NUNSqtiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Returning to our Hylleraas Helium wavefunction, we can evaluate the required updates to the parameters using `optimizer.update`:"
      ],
      "metadata": {
        "id": "NeqOoKCsrc6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# where gradients is the gradients of the energy with respect to the parameters,\n",
        "# as evaluated in problem 5.\n",
        "updates, opt_state = optimizer.update(gradients, opt_state, params)"
      ],
      "metadata": {
        "id": "dknwbu5YrdXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and calculate the new parameters using `optax.apply_updates`:"
      ],
      "metadata": {
        "id": "dLpiQJm1s4wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_params = optax.apply_updates(params, updates)"
      ],
      "metadata": {
        "id": "0EUVbUp5s4OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 6\n",
        "---\n",
        "Write a training loop for the Hylleraas Helium wavefunction which for each iteration:\n",
        "\n",
        "1. runs Metropolis-Hastings to draw samples according to $\\Psi^2$\n",
        "1. evaluates the energy and gradients\n",
        "1. updates the parameters\n",
        "\n",
        "Monitor the energy each iteration of the training loop to study convergence. Do you find parameters giving a lower energy ('better') wavefunction than the published values?\n",
        "\n",
        "Tip: Remember to equilibrate the MCMC samples after initialising them from a Gaussian distribution, as well as between each parameter update!"
      ],
      "metadata": {
        "id": "9APKkm6fSGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "e_compare = -2.90118\n",
        "plt.plot(np.arange(iterations), losses)\n",
        "plt.axhline(e_compare)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"E - E_compare\")\n",
        "plt.ylim(-2.91, -2.88)\n",
        "print(np.mean(losses[-10:]))"
      ],
      "metadata": {
        "id": "Q1NHcnHmbk41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple MLP for a 2e system\n"
      ],
      "metadata": {
        "id": "RevFLXgkF8Cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have learned almost everything we need: we can draw samples using Metropolis-Hastings, evaluate the energy and gradient using importance sampling, and update the parameters of a wavefunction and repeat!\n",
        "\n",
        "Now let's move on to using a neural network for the wavefunction. Let's continue with the helium atom. As the ground state is a singlet and helium is a two electron system, the spatial wavefunction must be symmetric (the antisymmetry comes from the spin part of the wavefunction). We can hence learn the helium ground state wavefunction using just a multi-layer perceptron (MLP)."
      ],
      "metadata": {
        "id": "WRXVCW4OWWta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure to get rid of everything related to the hylleraas wavefunction\n",
        "\n",
        "del total_energy, loss_and_grad, local_energy, pos, prob, metropolis, step_size\n",
        "del batch_size, params, mcmc_steps, losses"
      ],
      "metadata": {
        "id": "y-LageoYnJqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 7:\n",
        "\n",
        "Write a network which uses a simple envelope function multiplied by the MLP output, given by\n",
        "\\begin{equation}\n",
        "\\psi_{MLP}(\\vec r_1, \\vec r_2; \\theta) = e^{-Z(r_1 + r_2)} \\Phi(\\vec f(\\vec r_1, \\vec r_2); \\theta)\n",
        "\\end{equation}\n",
        "Here $Z=2$, and the envelope ensures the wavefunction obeys the open boundary conditions ($\\vec r \\to \\infty \\Rightarrow \\psi \\to 0$). The MLP is $\\Phi(\\vec f(\\vec R); \\theta)$, where $f$ constructs the input features to the network. Optimize the network using ADAM and monitor energy convergence.\n",
        "\n",
        "Try using electron positions and distances as features, $\\vec f(\\vec r_1, \\vec r_2) = \\{\\vec r_1, \\vec r_2, r_1, r_2\\}$, then try adding the electron-electron distances $r_{12} = |\\vec r_1 - \\vec r_2|$ as well. \n",
        "\n",
        "Some points:\n",
        "  - Don't worry if you can't beat the 4-parameter wavefunction, it is possible but requires hyperparameter tuning. The Hylleraas wavefunction parametrization is in fact quite good! \n",
        "  - Monitor pmove! Weird behaviour in the energy might easily come from pmove going to zero. \n",
        "  - Make sure enough mcmc steps are performed after initializing the electrons.\n",
        "  - Make sure enough mcmc steps are performed between each energy evaluation."
      ],
      "metadata": {
        "id": "ToHRZGscWge_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chex\n",
        "from typing import Dict, List\n",
        "LayerParams = Dict[str, jnp.ndarray]\n",
        "NetworkParams = List[LayerParams]\n",
        "\n",
        "# For first order optimization, parameter initialization matters\n",
        "def layer_params(key: chex.PRNGKey, m: int, n: int) -> LayerParams:\n",
        "  \"\"\"Glorot initialization, suitable for the tanh activation function.\"\"\"\n",
        "  bound = np.sqrt(6/(m + n)) \n",
        "  w_key, b_key = jax.random.split(key)\n",
        "  return dict(w=jax.random.uniform(w_key, shape = (n,m), minval=-bound, maxval=bound),\n",
        "             b=jnp.zeros(n))\n",
        "\n",
        "\n",
        "def init_mlp_params(key, hidden_layer_sizes: List[int], input_size: int =3) -> NetworkParams:\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    key: jax.PRNGkey\n",
        "    hidden_layer_sizes: list of the sizes of each hidden layer.\n",
        "  \"\"\"\n",
        "  sizes = [input_size] + list(hidden_layer_sizes) + [1]\n",
        "  # hidden layers\n",
        "  params = []\n",
        "  for m, n in zip(sizes[:-1], sizes[1:]):\n",
        "    key, subkey = jax.random.split(key)\n",
        "    params.append(layer_params(subkey, m, n))\n",
        "  return params\n",
        "\n",
        "\n",
        "def mlp_forward(inputs: jnp.ndarray, params: NetworkParams) -> jnp.ndarray:\n",
        "  \"\"\"Forward pass of the MLP.\n",
        "  \n",
        "  Args:\n",
        "    inputs: input features\n",
        "    params: list of dictionaries with the network parameters. \n",
        "  \"\"\"\n",
        "  a = inputs\n",
        "\n",
        "  for layer in params[:-1]:\n",
        "    outputs = jnp.dot(layer['w'], a) + layer['b']\n",
        "    a = jnp.tanh(outputs)\n",
        "\n",
        "  final_layer = params[-1]\n",
        "  return jnp.squeeze(jnp.dot(final_layer['w'], a))\n"
      ],
      "metadata": {
        "id": "frX-tVm5Svvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implement here"
      ],
      "metadata": {
        "id": "dxUR0IW3MZqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting code\n",
        "e_exact = -2.903724377\n",
        "plt.plot(losses, label='network loss')\n",
        "plt.axhline(e_compare, c='k', linestyle='--', label='4par, C. W. David')\n",
        "plt.axhline(e_exact, c='k', label='Exact Helium energy')\n",
        "plt.ylim(-2.907, -2.894)\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"E\")\n",
        "plt.grid()\n",
        "plt.figure()\n",
        "# pmove typically varies as the wavefunction varies, but should land around 50%\n",
        "# with this step size of 0.3\n",
        "plt.plot(pmoves)\n",
        "plt.ylabel(\"Move acceptance rate\")\n",
        "plt.xlabel(\"iteration\")\n",
        "print(np.mean(losses[-200:]))"
      ],
      "metadata": {
        "id": "tw66vMdV30R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Things we did not do.\n",
        "The network at hand should have sufficient capacity for the Helium atom. Here are some approaches that we could have taken to help convergence:\n",
        "  - gradient clipping, clipping outliers in the gradient of the log wavefunction, which can blow up near nodes of the wavefunction, causing unstable optimization.\n",
        "  - increase size of network (can help up to a point)\n",
        "  - second order optimization methods based on natural gradient descent such as KFAC, conjugate gradient etc. to help converge faster and more stably"
      ],
      "metadata": {
        "id": "8OXF2_H44VGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus: Implementing a minimal FermiNet\n",
        "\n",
        "Note: in the following $[a,b]$ denotes vector concatenation of $a$ and $b$.\n",
        "\n",
        "FermiNet consists of repeating layers of permutation-equivariant blocks and non-linear transformations:\n",
        "\n",
        "$$\n",
        "f^l_i = \\left[h^l_i, \\sum_{j,\\sigma_j\\in\\uparrow} h^l_j, \\sum_{j,\\sigma_j\\in\\downarrow} h^l_j\\right] \\\\\n",
        "h^l_i = \\tanh\\left(V^l f^{l-1} + g^l\\right) + h^{l-1}_i\n",
        "$$\n",
        "\n",
        "where the restrictions over the summations in $f^l_i$ indicate we average over the latent vectors associated with $\\uparrow$ electrons separately from those associated with $\\downarrow$ electrons, and\n",
        "\n",
        "$$\n",
        "h^0_i = [\\vec{r}_i, \\vec{r}_i]\n",
        "$$\n",
        "\n",
        "After $L$ layers, we need to produce $N_\\uparrow$ orbitals from each latent vector associated with a $\\uparrow$ electron and similarly for $\\downarrow$ electrons:\n",
        "\n",
        "$$\n",
        "\\phi_i(\\vec r_j) = (w_i h_j + g_i) . \\sum_I \\pi_{iI} \\exp(-\\Sigma_{iI} r_{iI})\n",
        "$$\n",
        "\n",
        "where the summation term gives the envelope function used to ensure the boundary conditions are satisfied. We can hence evaluate the spin-factored wavefunction:\n",
        "\n",
        "$$\n",
        "\\Psi = \\det[\\phi_i^\\uparrow] \\det[\\phi_j^\\downarrow]\n",
        "$$\n",
        "\n",
        "Implement and train this model on a small atom such as Lithium or Beryllium.\n",
        "\n",
        "## Bonus Bonuses\n",
        "\n",
        "### Two electron streams\n",
        "\n",
        "We can extend the minimal FermiNet model to include two-electron features explicitly using a two electron functions, which we model with a separate MLP:\n",
        "\n",
        "$$\n",
        "h^0_{ij} = [\\vec{r}_{ij}, r_{ij}] \\\\\n",
        "h^l{ij} = \\tanh(W^l h^{l-1}_{ij} + c^l) + h^{l-1}_{ij}\n",
        "$$\n",
        "\n",
        "The activation to the $l$-th layer of the one-electron then becomes:\n",
        "\n",
        "$$\n",
        "f_i^l = \\left[h^l_i, \\sum_{j,\\sigma_j\\in\\uparrow} h^l_j, \\sum_{j,\\sigma_j\\in\\downarrow} h^l_j, \\sum_{ij,\\sigma_j=\\sigma_j} h^l_{ij}, \\sum_{ij,\\sigma_j=\\sigma_j} h^l_{ij} \\right]\n",
        "$$\n",
        "\n",
        "### Determinant expansions\n",
        "\n",
        "We can create a linear combination of $N_k$ FermiNet determinants by producing $N_k$ more orbitals in the orbital shaping linear transformation. Extend your model to handle multiple determinants.\n",
        "\n",
        "Note: it might be numerically more stable to work in the log domain here!"
      ],
      "metadata": {
        "id": "X7xDazzR7P_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## coupled cluster\n",
        "If you do well with the MLP or do the bonus questions, you might want a better reference value. The code snippet below evaluates the Coupled Cluster Singles Doubles (CCSD) Helium energy for a fairly large basis of Gaussian Type Orbitals (GTOs). CCSD contains all excitations up to double-excitations, so it is exact (in the given basis set) for two particles."
      ],
      "metadata": {
        "id": "YqDJhzw020rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyscf\n",
        "import pyscf.cc\n",
        "m = pyscf.gto.mole.M(atom = \"He 0 0 0\", basis=\"ccpv5z\")\n",
        "m = m.build()\n",
        "\n",
        "mf = pyscf.scf.RHF(m)\n",
        "mf.kernel()\n",
        "mycc = pyscf.cc.CCSD(mf).run()\n",
        "e_cc = mycc.e_tot"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VQoX7xZh2_Jm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4db2671-9158-43b6-cd43-88ad279c6694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "converged SCF energy = -2.86162483458191\n",
            "E(CCSD) = -2.903151880049923  E_corr = -0.04152704546801235\n"
          ]
        }
      ]
    }
  ]
}